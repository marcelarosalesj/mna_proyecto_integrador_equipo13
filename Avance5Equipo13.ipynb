{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "72e00b22858044fab75a9c91111f1c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_303b219a5da348ab93fdbef0be6079cb",
              "IPY_MODEL_69211095d6114bada832c8dfde3fd585",
              "IPY_MODEL_389c1e4f78cf4f9c87fcd41d9a4aea25"
            ],
            "layout": "IPY_MODEL_3284bcdc0c7f4d57b8e2df67647b3784"
          }
        },
        "303b219a5da348ab93fdbef0be6079cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7755ecc9b689480cb8e4e779bcb7432b",
            "placeholder": "​",
            "style": "IPY_MODEL_e5bc537ea5554ee3936d1731abb03dfb",
            "value": "config.json: 100%"
          }
        },
        "69211095d6114bada832c8dfde3fd585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74e8a4c2cc794023a1e7f77db4492834",
            "max": 1154,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdd322d9a8d2409592f64b40eda42576",
            "value": 1154
          }
        },
        "389c1e4f78cf4f9c87fcd41d9a4aea25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1df438ae6e7a41b693555a92e5fe0a20",
            "placeholder": "​",
            "style": "IPY_MODEL_2f387c85c1614a1487fe546f8cd2c926",
            "value": " 1.15k/1.15k [00:00&lt;00:00, 94.0kB/s]"
          }
        },
        "3284bcdc0c7f4d57b8e2df67647b3784": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7755ecc9b689480cb8e4e779bcb7432b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5bc537ea5554ee3936d1731abb03dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74e8a4c2cc794023a1e7f77db4492834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdd322d9a8d2409592f64b40eda42576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1df438ae6e7a41b693555a92e5fe0a20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f387c85c1614a1487fe546f8cd2c926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "151e5c45353543abb01953a0e4446544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09f204564e7c4209af77908a2bfdccef",
              "IPY_MODEL_7af8dcd3fa404082a67587c3fd659956",
              "IPY_MODEL_d535fcca763d498297fcee1386bf6a67"
            ],
            "layout": "IPY_MODEL_b60f8ece4a4d4aaba1b5ddb38746cd9d"
          }
        },
        "09f204564e7c4209af77908a2bfdccef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a3723fce3c04394a41cac438ef6917e",
            "placeholder": "​",
            "style": "IPY_MODEL_bbece02895b746e987166e96bf65b64c",
            "value": "model.safetensors: 100%"
          }
        },
        "7af8dcd3fa404082a67587c3fd659956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f932a6b860b345a18bfb84107a0ca172",
            "max": 1629437147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f986ba487f6b4aa3824b4a423f306d50",
            "value": 1629437147
          }
        },
        "d535fcca763d498297fcee1386bf6a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ffda4f46b30439692ed74cb6e049588",
            "placeholder": "​",
            "style": "IPY_MODEL_9066121fee874fb890f3bfc4a3badb24",
            "value": " 1.63G/1.63G [00:07&lt;00:00, 228MB/s]"
          }
        },
        "b60f8ece4a4d4aaba1b5ddb38746cd9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a3723fce3c04394a41cac438ef6917e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbece02895b746e987166e96bf65b64c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f932a6b860b345a18bfb84107a0ca172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f986ba487f6b4aa3824b4a423f306d50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ffda4f46b30439692ed74cb6e049588": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9066121fee874fb890f3bfc4a3badb24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cdea4f9c01042a3a76cb838bd951ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10ee94b4f1d44d518353a7bdc3702c5c",
              "IPY_MODEL_392210b46c544433ad0a283d36dbe5f3",
              "IPY_MODEL_1cb332f82d944c4fb2ab531803d97bfd"
            ],
            "layout": "IPY_MODEL_0ca78f80cd0d4e1ab09013add09469d5"
          }
        },
        "10ee94b4f1d44d518353a7bdc3702c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aafdd411e4c495a93043d651334861e",
            "placeholder": "​",
            "style": "IPY_MODEL_7016869df29d40338ac629bfbfe39e6a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "392210b46c544433ad0a283d36dbe5f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bc8073ff6c6482fb4fe01f7d6a9205c",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c11cfed723a64f1e9885e3992e7e2727",
            "value": 26
          }
        },
        "1cb332f82d944c4fb2ab531803d97bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7a762bbf5e546e2bc21a78bd4fa5c39",
            "placeholder": "​",
            "style": "IPY_MODEL_147cbf3f7efa4d9d8786d2a6c06dd14a",
            "value": " 26.0/26.0 [00:00&lt;00:00, 2.32kB/s]"
          }
        },
        "0ca78f80cd0d4e1ab09013add09469d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aafdd411e4c495a93043d651334861e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7016869df29d40338ac629bfbfe39e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bc8073ff6c6482fb4fe01f7d6a9205c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c11cfed723a64f1e9885e3992e7e2727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7a762bbf5e546e2bc21a78bd4fa5c39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "147cbf3f7efa4d9d8786d2a6c06dd14a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b91224d75e6841718066ba9d4936dae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4e6b40f643848a5a9778c38855b16cd",
              "IPY_MODEL_825f5204d2674bd68a3c8c3d20e23dda",
              "IPY_MODEL_ae2bc7c7c0844b0e91cbb236198cb020"
            ],
            "layout": "IPY_MODEL_a512b5d3dc4d4e5897725443cf7a3a13"
          }
        },
        "b4e6b40f643848a5a9778c38855b16cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_993ee3485be8427fa5be70029a4c2b6e",
            "placeholder": "​",
            "style": "IPY_MODEL_953a00d37d1a4e42943d6d7dc1ab0e31",
            "value": "vocab.json: 100%"
          }
        },
        "825f5204d2674bd68a3c8c3d20e23dda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_814136ee3c6847768e89906d9b9daaf6",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5a3c77847b448ecad38bb01c5b75d3c",
            "value": 898822
          }
        },
        "ae2bc7c7c0844b0e91cbb236198cb020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33f59d2f04e34fe8ace9a231a9aa5eb1",
            "placeholder": "​",
            "style": "IPY_MODEL_90e63832924049de8784bead117d73e4",
            "value": " 899k/899k [00:00&lt;00:00, 1.39MB/s]"
          }
        },
        "a512b5d3dc4d4e5897725443cf7a3a13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "993ee3485be8427fa5be70029a4c2b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "953a00d37d1a4e42943d6d7dc1ab0e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "814136ee3c6847768e89906d9b9daaf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5a3c77847b448ecad38bb01c5b75d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33f59d2f04e34fe8ace9a231a9aa5eb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90e63832924049de8784bead117d73e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bcf5fc10aa0466887d58d62308493a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1356f9c3ea24f51b122072cc5c18323",
              "IPY_MODEL_d1a89270dceb4739a11ad5dbd375f33a",
              "IPY_MODEL_6d18a1a309a84ebb8b340eeb3d97d5de"
            ],
            "layout": "IPY_MODEL_cadb03b57071415792de9ed72030821d"
          }
        },
        "d1356f9c3ea24f51b122072cc5c18323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2def0a3eee704d91859307317f7297bb",
            "placeholder": "​",
            "style": "IPY_MODEL_9a1aa800c79748929421a73cb2caae96",
            "value": "merges.txt: 100%"
          }
        },
        "d1a89270dceb4739a11ad5dbd375f33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29b85452178c4a00a3d715f8401905a3",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_02f86163aef64d67a121b9c932193fc7",
            "value": 456318
          }
        },
        "6d18a1a309a84ebb8b340eeb3d97d5de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2efe9b3ae6b4ef2b4715a0df8c1e727",
            "placeholder": "​",
            "style": "IPY_MODEL_4bd51cb48e3a41d2a3e8b41aaba83538",
            "value": " 456k/456k [00:00&lt;00:00, 2.11MB/s]"
          }
        },
        "cadb03b57071415792de9ed72030821d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2def0a3eee704d91859307317f7297bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a1aa800c79748929421a73cb2caae96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29b85452178c4a00a3d715f8401905a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02f86163aef64d67a121b9c932193fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2efe9b3ae6b4ef2b4715a0df8c1e727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bd51cb48e3a41d2a3e8b41aaba83538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74c37769ad1142ff838f70bae34ffe58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38db0dc3b729423bbd78b67272297cc8",
              "IPY_MODEL_6e01959cce0a4b40870af8587f243d83",
              "IPY_MODEL_5a86880e1bfb4b6dbd3d992c10fcc264"
            ],
            "layout": "IPY_MODEL_5f3076c54a9f4be5ad6fb78f6a30e497"
          }
        },
        "38db0dc3b729423bbd78b67272297cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f36abb325dd344e08ef0ee2b27f66c3b",
            "placeholder": "​",
            "style": "IPY_MODEL_b9b995e4f27140f2b814222151d1abf6",
            "value": "tokenizer.json: 100%"
          }
        },
        "6e01959cce0a4b40870af8587f243d83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d8d4dc7b944470eb2b29ae1570052d3",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb4e02c71d7b4c4dbf2ab04d29261f22",
            "value": 1355863
          }
        },
        "5a86880e1bfb4b6dbd3d992c10fcc264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d99e7db7f87b4d65bbbf4890072cd6ba",
            "placeholder": "​",
            "style": "IPY_MODEL_7e5d1d9a0ccd4ea09f0823862dfa7471",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 6.24MB/s]"
          }
        },
        "5f3076c54a9f4be5ad6fb78f6a30e497": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f36abb325dd344e08ef0ee2b27f66c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9b995e4f27140f2b814222151d1abf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d8d4dc7b944470eb2b29ae1570052d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb4e02c71d7b4c4dbf2ab04d29261f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d99e7db7f87b4d65bbbf4890072cd6ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e5d1d9a0ccd4ea09f0823862dfa7471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Proyecto Integrador - Avance 5. Modelo Final**\n",
        "## **Tecnologico de Monterrey**\n",
        "------------------------------------------------------------------\n",
        "### Profa. Dra. Grettel Barceló Alonso\n",
        "\n",
        "### Prof. Dr. Luis Eduardo Falcón Morales\n",
        "\n",
        "### Profa. Verónica Sandra Guzmán de Valle\n",
        "------------------------------------------------------------------\n",
        "### Marcela Alejandra Rosales Jiménez - A01032022\n",
        "### José Antonio Mendoza Castro - A01794067"
      ],
      "metadata": {
        "id": "vibkFZBTBmSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Preparación de ambiente**"
      ],
      "metadata": {
        "id": "EGA-F5_PBq3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalacion de librerias"
      ],
      "metadata": {
        "id": "qNHUb4qMBxU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aSxwGtKj4ri",
        "outputId": "59a5a904-04e9-44d2-939e-c2fdff379479"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Librerias"
      ],
      "metadata": {
        "id": "jYhB_tVwB0z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "1ubP0ZYujIv-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "metadata": {
        "id": "5wNpEfS_w2Mn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Carga de datos**"
      ],
      "metadata": {
        "id": "Rz_KGQQACIl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('captions_dataset.csv')"
      ],
      "metadata": {
        "id": "xStg1Hctp037"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Funciones**"
      ],
      "metadata": {
        "id": "pvCbj4OmB7F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords(text):\n",
        "  \"\"\"\n",
        "  Funcion que extrae los sustantivos, verbos y adjetivos de un texto.\n",
        "  \"\"\"\n",
        "  doc = nlp(text)\n",
        "  keywords = []\n",
        "  for token in doc:\n",
        "    if token.pos_ in ('NOUN', 'VERB', 'ADJ'):\n",
        "      keywords.append(token.text)\n",
        "  return keywords"
      ],
      "metadata": {
        "id": "Dpv1OrG0Fafj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text(text, candidate_labels_tmp):\n",
        "  \"\"\"\n",
        "  Funcion que clasifica un texto en base a una lista de etiquetas.\n",
        "  \"\"\"\n",
        "  result = classifier(text, candidate_labels_tmp)\n",
        "  return result['labels'][0]"
      ],
      "metadata": {
        "id": "Vs18Mz6s53kl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Exploracion de los datos**"
      ],
      "metadata": {
        "id": "J5JFBC3kCX2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizamos algunos de los datos, contamos con 8 columnas. La columna id nos permite identificar los textos, para cada id hay 5 textos correspondientes. La columna video_name es el nombre del video al que le corresponde ese texto. El campo fps representa frames/second. La columna label representa la etapa de cada video, en total por cada video hay 5 etapas: precognition, recognition,judgement, action y avoidance. Las columnas caption_pedestrian y caption_vehicle se refieren a la perspectiva del texto, el primero corresponde al peaton y el segundo al vehiculo. Finalmente las columnas start_time y end_time corresponden al segundo de inicio y fin al que corresponde esa etapa."
      ],
      "metadata": {
        "id": "MZ8mCWxCDdTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[20:40]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rK9tJKSwwvoI",
        "outputId": "6b1dc009-070a-4f82-c36a-bc96e26ba93e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      id     video_name    fps  \\\n",
              "20  9fa4da9c-4b60-48d9-a159-53547b02aedf  video3187.mp4  29.97   \n",
              "21  9fa4da9c-4b60-48d9-a159-53547b02aedf  video3187.mp4  29.97   \n",
              "22  9fa4da9c-4b60-48d9-a159-53547b02aedf  video3187.mp4  29.97   \n",
              "23  9fa4da9c-4b60-48d9-a159-53547b02aedf  video3187.mp4  29.97   \n",
              "24  9fa4da9c-4b60-48d9-a159-53547b02aedf  video3187.mp4  29.97   \n",
              "25  99934c50-d5d3-4c44-a9e3-9450fe8e1ceb  video1248.mp4  30.00   \n",
              "26  99934c50-d5d3-4c44-a9e3-9450fe8e1ceb  video1248.mp4  30.00   \n",
              "27  99934c50-d5d3-4c44-a9e3-9450fe8e1ceb  video1248.mp4  30.00   \n",
              "28  99934c50-d5d3-4c44-a9e3-9450fe8e1ceb  video1248.mp4  30.00   \n",
              "29  99934c50-d5d3-4c44-a9e3-9450fe8e1ceb  video1248.mp4  30.00   \n",
              "30  79a3074c-651f-4726-882c-a64c38d42b6b   video157.mp4  29.97   \n",
              "31  79a3074c-651f-4726-882c-a64c38d42b6b   video157.mp4  29.97   \n",
              "32  79a3074c-651f-4726-882c-a64c38d42b6b   video157.mp4  29.97   \n",
              "33  79a3074c-651f-4726-882c-a64c38d42b6b   video157.mp4  29.97   \n",
              "34  79a3074c-651f-4726-882c-a64c38d42b6b   video157.mp4  29.97   \n",
              "35                                  1663            NaN    NaN   \n",
              "36                                  1663            NaN    NaN   \n",
              "37                                  1663            NaN    NaN   \n",
              "38                                  1663            NaN    NaN   \n",
              "39                                  1663            NaN    NaN   \n",
              "\n",
              "             label  \\\n",
              "20  prerecognition   \n",
              "21     recognition   \n",
              "22       judgement   \n",
              "23          action   \n",
              "24       avoidance   \n",
              "25  prerecognition   \n",
              "26     recognition   \n",
              "27       judgement   \n",
              "28          action   \n",
              "29       avoidance   \n",
              "30  prerecognition   \n",
              "31     recognition   \n",
              "32       judgement   \n",
              "33          action   \n",
              "34       avoidance   \n",
              "35               4   \n",
              "36               3   \n",
              "37               2   \n",
              "38               1   \n",
              "39               0   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             caption_pedestrian  \\\n",
              "20                                                                                    The pedestrian, a middle-aged male in his 50s, stood in an urban setting on a bright weekday morning. He was wearing a blue T-shirt and slacks, his height measuring around 170 cm. With glasses perched on his nose and a black hat atop his head, he appeared to be closely watching his surroundings. Unaware of the vehicle directly in front of him, the pedestrian's body was positioned perpendicular to the vehicle and to the right. His line of sight was fixed straight ahead, aligned with the direction of travel. The pedestrian seemed to be in no rush, moving slowly as he attempted to cross the road. The conditions were optimal, with the road surface dry and the weather clear. Despite the light traffic volume on the main road, there was only one lane available for vehicles traveling in one direction. Sidewalks were present on both sides, adding to the pedestrian's sense of safety and ease as he made his way across.   \n",
              "21                                                                                                                                                                                            The pedestrian is a middle-aged man in his 50s with a height of 170 cm. He is wearing a black hat, a blue T-shirt as an upper body clothing, and blue slacks as lower body clothing. He is also wearing glasses. The event takes place in an urban area on a clear weekday. The weather is bright and the road conditions are dry. The road surface is made of asphalt and there is only one lane on the one-way main road. There is a sidewalk on both sides. The pedestrian is standing directly in front of a vehicle while facing perpendicular to it and to the right. He is closely watching the vehicle and is almost aware of its presence. His line of sight is in front, aligning with the vehicle's direction of travel. He is slowly crossing the road and appears to be cautious. The traffic volume is light and the road is level.   \n",
              "22                                                                                      The pedestrian, a man in his 50s wearing a blue T-shirt and slacks, with a black hat and glasses, stands directly in front of the vehicle on the urban main road on a clear and bright weekday. His body is positioned perpendicular to the vehicle and to the right. He is close to the vehicle, and his line of sight is in front, in the direction of travel. The pedestrian closely watches his surroundings, almost noticing the presence of the vehicle. He is aware of the vehicle and slowly crosses the road. The road surface is dry and level, with light traffic volume. There is one-way traffic on the road, with a single lane and sidewalks on both sides. The environment presents a calm and steady atmosphere as the pedestrian cautiously proceeds with his crossing, taking into account the surroundings and his own visual state. Overall, the situation remains under control and harmonious, with no immediate danger or rush.   \n",
              "23                                                                                                                                                            The pedestrian stands perpendicular to the vehicle and to the right. Positioned directly in front of the vehicle, the pedestrian is relatively close to it. Their line of sight is in front, aligned with the direction of travel. Closely watching, the pedestrian's visual status indicates their awareness of the surroundings. Moving slowly, the pedestrian is making their way across. In terms of the environment condition, the pedestrian is a male in his 50s, with a height of 170 cm. He wears glasses and a black hat, and his upper body is adorned with a blue t-shirt. His lower body is clad in blue slacks. Taking place in an urban setting on a clear and bright weekday, the road surface is dry and level, made of asphalt. The traffic volume is light, and it is a one-way, one-lane road with sidewalks on both sides. The event unfolds on a main road.   \n",
              "24                                                                                                                                                                   The pedestrian, a middle-aged man in his 50s, was standing directly in front of the vehicle on a clear weekday. He was relatively close to the vehicle, with his body perpendicular and to the right. Closely watching, he had a line of sight in front, following the direction of the vehicle's travel. Although he had almost noticed the vehicle's presence, he was slowly crossing the road. The man's appearance included glasses and he was dressed in a blue T-shirt and slacks, with a black hat. The urban environment offered bright lighting, with dry and level asphalt road conditions. The main road had light traffic with only one lane, and sidewalks were present on both sides. These details painted a clear picture of the pedestrian's situation, providing important information on his relative position to the vehicle and overall surroundings.   \n",
              "25                                                                                                                                                                                      The pedestrian, a woman in her 40s, stands still on a clear, bright weekday morning. She is wearing a purplish red T-shirt and a gray skirt, her height measuring approximately 160 cm. Oblivious to the vehicle nearby, she closely watches her intended crossing destination. The pedestrian's body is oriented perpendicularly to the left side of the vehicle while positioning herself diagonally to the right and in front of it. In this urban environment, on a level, dry asphalt road with usual traffic volume, the main road consists of a one-way lane with sidewalks on both sides. The pedestrian seems unaware of the vehicle's presence, as she remains engrossed in her surroundings. Despite the busy traffic and the potential danger it poses, she stands still, perhaps waiting for the opportune moment to move across the road.   \n",
              "26                                                                                                                                                                                                                                                                                                              A female pedestrian in her 40s with a height of 160 cm was standing still on a main road. She was wearing a purplish red T-shirt and a gray skirt. The pedestrian's body was perpendicular to the vehicle and positioned diagonally to the right, in front of the vehicle. She had a close line of sight to her crossing destination and was closely watching her surroundings. Although she was almost noticed, she was aware of the vehicle. The pedestrian was in an urban area on a weekday, with clear weather and bright lighting. The road surface conditions were dry and level, with asphalt as the road surface type. The traffic volume was usual on this one-way, one-lane road, which had sidewalks on both sides.   \n",
              "27                                                        The pedestrian, a female in her 40s, stood still diagonally to the right in front of the vehicle on a clear and bright weekday. She was wearing a purplish red T-shirt and a gray skirt, standing on the dry asphalt road surface. The road was a main road with only one way and one lane, and there were sidewalks on both sides. The pedestrian's body was oriented diagonally to the left, opposite to the direction of the vehicle. She closely watched the road surface, indicating her visual focus. Although she had almost noticed the vehicle, she was unaware of its presence. The relative distance between the pedestrian and the vehicle was close. The overall environment conditions were urban, and the traffic volume was usual. The pedestrian's line of sight was unobstructed, and the road surface was level. These details provide a clear understanding of the situation surrounding the pedestrian and the environment they were in at that particular time.   \n",
              "28                                                                                                                                                                                                                                          The pedestrian, a woman in her 40s, stood diagonally to the right and in front of the vehicle. She closely watched the vehicle, almost noticed its presence. Her body was oriented diagonally to the left, opposite to the direction of the vehicle. With her line of sight fixed on her crossing destination, she appeared ready to cross. Moving slowly, she stood in front of the vehicle, seemingly aware of its presence. The woman's clothing consisted of a purplish red T-shirt on her upper body and a gray skirt on her lower body. The urban environment was characterized by a clear and bright weather on a dry asphalt road, with usual traffic volume. The road was a main road with only one lane in the direction the pedestrian was facing. Sidewalks were present on both sides.   \n",
              "29                                                                                  The pedestrian stood perpendicular to the vehicle and to the left. They were positioned directly in front of the vehicle, at a close relative distance. Their line of sight was focused on their crossing destination, and they closely watched the surroundings. Moving slowly, they were heading across the road. The pedestrian, an adult female in her 40s, stood at a height of 160 cm. She was wearing a purplish red T-shirt, and a gray skirt. The event took place in an urban area on a clear weekday with bright lighting conditions. The road surface was dry and level, made of asphalt. It was a main road with one-way traffic and one lane, with sidewalks available on both sides. The traffic volume was normal, and the surroundings were typically suburban. This information provides a comprehensive description of the pedestrian's orientation, appearance, location, and the environmental conditions in which the event occurred.   \n",
              "30                                                                                                                                                                                                                                    The pedestrian is a female in her 30s, standing still on a main road in an urban area. She is wearing a dark green jacket, slacks, and a dark green hat. The weather is snowy, with bright brightness. The road surface is frozen, but there is no incline. The road is an asphalt surface with one-way traffic and three lanes. The pedestrian's body is oriented diagonally to the left, which is opposite to the direction of the vehicle. She is positioned diagonally to the right, in front of the vehicle. The pedestrian's line of sight is in front, following the direction of travel. She is closely watching her surroundings. However, she is unaware of the vehicle approaching her. Both sides of the road have sidewalks. This event occurs on a weekday and the traffic volume is usual.   \n",
              "31                         The pedestrian, a female in her 30s wearing a dark green jacket and slacks, stood still diagonally to the right in front of the vehicle on a bright snowy weekday morning. She was closely watching and noticed the vehicle approaching from the opposite direction diagonally to the left. Her line of sight was in front, in the direction of travel. The pedestrian's body was oriented diagonally to the left, opposite to the vehicle's direction. The road surface was frozen and inclined on the main road, which had one-way, three-lane traffic. The urban environment had sidewalks on both sides. Despite the snowy weather, the brightness was high, providing good visibility. This event occurred in an urban setting with usual traffic volume and an asphalt road surface. The pedestrian's clothing also included a dark green hat, matching the upper body's clothing color. Overall, the pedestrian's actions, visual status, and awareness of the vehicle were observed and described in detail.   \n",
              "32                                                                                                                                                                                                                                                                                                                                           A pedestrian, a woman in her 30s, stands still diagonally to the right, in front of the vehicle. She is oriented diagonally to the left, opposite to the vehicle's direction. Her line of sight is in front, following the vehicle's path. Closely watching, she notices the vehicle and is aware of its presence. The pedestrian's clothing consists of a dark green jacket, dark green hat, and black slacks. The environment is urban and it is a weekday. Despite the bright snowfall, the visibility is clear. The road surface conditions are frozen on a level asphalt road. The traffic volume is usual on this one-way main road with three lanes, and there are sidewalks on both sides.   \n",
              "33                                                                                                                                                                                  The pedestrian, a female in her 30s, stood still on the frozen asphalt road of the urban environment. Her body was oriented diagonally to the left, opposite to the direction of an approaching vehicle. Positioned diagonally to the right in front of the vehicle, she closely watched its movements. Despite the bright snowy weather, her line of sight was focused in the direction of travel. The pedestrian, dressed in a dark green jacket and slacks, also wore a dark green hat. The road, a main one-way thoroughfare with three lanes, had sidewalks on both sides. It was a weekday, and traffic volume was usual. The surroundings were illuminated, and the road surface conditions were icy. Amidst these circumstances, the pedestrian, standing still, noticed the vehicle but did not seem to display any immediate or specific actions.   \n",
              "34                                                                                                                                   The pedestrian consists of a female in her 30s, with a height of 170 cm. She is dressed in a dark green jacket and slacks, wearing a dark green hat. Currently, she is located diagonally to the right and in front of a vehicle, standing still. Despite the snowy weather with bright lighting, the pedestrian closely watches the vehicle, as she notices its presence. Her line of sight is directed in front, aligned with the vehicle's direction of travel. The road conditions are frozen, and the road surface type is asphalt. The environment in which this event takes place is urban, with an average traffic volume on a main road consisting of one-way access and three lanes. Additionally, there are sidewalks available on both sides. The event occurs on a weekday, and the pedestrian exhibits an orientation of her body, diagonally to the left, opposing the vehicle's direction.   \n",
              "35                                                                                        The pedestrian, a young man in his twenties, stands diagonally to the right of the vehicle, completely unaware of its presence. He is positioned directly in front of the car, close in distance. His body is motionless and his line of sight is immediately above. The pedestrian's attire consists of a black T-shirt and black slacks, matching the dark and cloudy weather. The road conditions are favorable, with the dry asphalt providing a level surface. The residential road intersects with a signal, indicating that both the vehicle and the pedestrian should exercise caution. Although the traffic volume is light, there are two-way lanes available. On this particular street, there are no sidewalks or roadside strips on both sides, but there are street lights illuminating the path. This simple event captures a snapshot of the pedestrian's surroundings and his lack of awareness towards the vehicle approaching him.   \n",
              "36  The pedestrian, a male in his 20s with a height of 170 cm, was standing still on a residential road intersection with signal. He was diagonally positioned to the right, in the same direction as the vehicle, and directly in front of it. Unaware of the vehicle's presence, his line of sight was immediately above. The pedestrian's body language showed no signs of abnormal activity, as he was calmly standing still. He was wearing a black T-shirt on his upper body and black slacks on his lower body. The weather was cloudy, resulting in a dark brightness. The road surface was dry and level, made of asphalt. The traffic volume was light on the two-way traffic residential road. There were no sidewalks on both sides, and neither were there roadside strips. However, street lights were available, providing illumination. The overall conditions of the environment indicate that the pedestrian was stationary in a potentially unsafe position, with limited visibility due to the darkness and cloudy weather.   \n",
              "37                                                                                                                                                                                                                  The pedestrian is a male in his 20s, approximately 170 cm tall, wearing a black T-shirt and black slacks. He is standing diagonally to the right, in front of the vehicle, with his body oriented diagonally to the right as well, in the same direction as the vehicle. The pedestrian is close to the vehicle, and his line of sight is immediately above. He is unaware of the vehicle's presence. The overall environment is cloudy and dark, with a dry asphalt road surface. The road is a residential road with two-way traffic and an intersection with a signal. There are no sidewalks on both sides, and there are no roadside strips; however, there are street lights present. The pedestrian's general action is to stand still, but his abnormal action is lying stretched out. The traffic volume is light.   \n",
              "38                                                                                                In a residential road intersection on a cloudy day, a male pedestrian in his 20s with a height of 170 cm stands diagonally to the right, in front of a vehicle. The pedestrian is wearing a black T-shirt and black slacks. With his line of sight immediately above, he remains unaware of the vehicle's presence, which is positioned near him. The road conditions are dry and level with a light traffic volume. The road surface is asphalt, and there are two lanes for two-way traffic. Although the surroundings are dark, street lights illuminate the area. The pedestrian is standing still, not moving or taking any abnormal actions. As far as the environment is concerned, there are no sidewalks on both sides, nor are there roadside strips on both sides. Despite the environmental factors and proximity to the vehicle, the pedestrian appears to be uninformed about its existence and continues to remain stationary.   \n",
              "39                                                                                                                   The pedestrian, a male in his 20s with a height of 170 cm, was wearing a black T-shirt and black slacks. It was a cloudy and dark day, with the road being dry and the traffic volume light. He was standing diagonally to the right, in front of a vehicle on a residential road intersection with a signal. The pedestrian's body orientation was the same direction as the vehicle, and his line of sight was immediately above. He was closely watching his surroundings, but unaware of the vehicle approaching him. Despite the pedestrian being near the vehicle, the pedestrian was standing still. However, an abnormal action was noticed as he was lying stretched out. The environment condition portrayed a typical urban setting, with asphalt as the road surface type and a two-way traffic road with not both sides having a sidewalk and roadside strip. Street lights were present in the surroundings.   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        caption_vehicle  \\\n",
              "20                                                                                                                                                                                                                                                                                                                                                                                                                                              The vehicle was moving at a constant speed when it suddenly came to a stop. Positioned on the right side of a pedestrian, the vehicle was in close proximity to them. The pedestrian was clearly visible within the vehicle's field of view. The vehicle's speed was at 0km/h, indicating a complete halt. In terms of the environment conditions, the pedestrian was a male in his 50s, standing at a height of 170 cm. He was wearing glasses, a black hat, a blue T-shirt, and blue slacks. The event took place in an urban area on a weekday with clear weather and bright brightness. The road surface was dry and level, consisting of asphalt. The traffic volume was light as the vehicle was traveling on a main road with a single one-way lane and sidewalks on both sides.   \n",
              "21                                                                                                                                                                                                                                                                                                             The vehicle is currently in a state of constant speed and is positioned on the right side of a pedestrian. They are situated in close proximity to each other, with the vehicle being able to see the pedestrian within its field of view. At this moment, the vehicle has come to a complete stop, with its speed at 0 km/h. As for the environmental conditions, the pedestrian is a male in his 50s, standing at a height of 170 cm. The pedestrian is wearing glasses and a black hat, along with a blue t-shirt and blue slacks. The event takes place in an urban area on a weekday. The weather is clear, and the brightness is bright. The road conditions are optimal, with the asphalt road surface being dry and level. This particular location is classified as a main road with a single one-way lane and sidewalks on both sides. The traffic volume is light, allowing for a relatively calm atmosphere.   \n",
              "22                                                                                                                                                                                                                                                                                                                                            The vehicle is currently stationary, maintaining a constant speed, with a close relative distance to a pedestrian on its right side. The vehicle's field of view allows it to see the pedestrian clearly. Analyzing the surroundings, it can be observed that the weather is clear, the brightness is bright, and the road surface conditions are dry. The vehicle is situated in an urban area, specifically on a main road with a single lane in the direction it is traveling. There are sidewalks on both sides of the road. The environment conditions indicate that the pedestrian is a male in his 50s, approximately 170 cm tall, wearing glasses and a black hat. His upper body is adorned with a blue T-shirt, and he is wearing blue slacks for his lower body. This event occurs on a weekday with light traffic volume on the road. The vehicle is placed on level asphalt.   \n",
              "23                                                                                                                                                                                                                                                                                                                                                                              The vehicle is currently experiencing constant speed as it moves along. Positioned on the right side of the pedestrian, it is close in relative distance. From its field of view, the pedestrian is visible to the vehicle. The vehicle is in a stopped state with a speed of 0km/h. In the surrounding environment, a male in his 50s is present. He has a height of 170 cm and wears glasses. Additionally, he sports a black hat, a blue T-shirt for his upper body, and blue slacks for his lower body. The setting of this event takes place in an urban area, specifically on a weekday. The weather is clear with bright lighting conditions. The road surface is dry and level, made of asphalt. It is a main road with one-way traffic and a single lane. Sidewalks are available on both sides. The traffic volume is light in this scenario.   \n",
              "24                                                                                                                                                                                                                                                                                                                                                                                                      The vehicle is currently experiencing acceleration and is positioned on the right side of a pedestrian. The vehicle is at a close distance from the pedestrian and has a clear field of view where it can see the pedestrian. The vehicle is about to start moving and is moving at a speed of 5km/h. The environment conditions show that the pedestrian is a male in his 50s, with a height of 170 cm. He is wearing glasses and a black hat, along with a blue T-shirt and blue slacks. The event is taking place in an urban setting, specifically on a weekday. The weather is clear and bright, with dry road surface conditions on a level asphalt road. The traffic volume is light and the road is classified as a main road with one-way traffic and one lane. There are sidewalks present on both sides of the road.   \n",
              "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                     A vehicle is decelerating while travelling diagonally to the left in front of a pedestrian. The vehicle is near the pedestrian and has a clear field of view with the pedestrian visible. It is moving straight ahead at a speed of 10km/h. The vehicle is in an urban environment and it is a weekday. The weather is clear and the brightness is bright. The road surface conditions are dry and level, with the road surface being asphalt. The traffic volume is normal and the road is classified as a main road with one way and one lane. There are sidewalks present on both sides of the road. As for the environment condition, the pedestrian is a female in her 40s with a height of 160 cm. She is wearing a purplish red T-shirt and a gray skirt.   \n",
              "26                                                                                                                                                                                                                                                                                                                                                                                                                                                 The vehicle is currently decelerating and is positioned diagonally to the left in front of the pedestrian. The vehicle is near to the pedestrian and has a clear view of them. It appears that the vehicle is about to stop. Additionally, the vehicle is traveling at a speed of 5km/h. As for the environmental conditions, the pedestrian is a female in her 40s, standing at a height of 160 cm. She is wearing a purplish red T-shirt and a gray skirt. The event takes place in an urban setting on a clear and bright weekday. The road surface is dry and level, with asphalt as the type of road surface. The traffic volume is usual, and the road is classified as a main road with one-way traffic flow and one lane. Sidewalks are available on both sides of the road.   \n",
              "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The vehicle is positioned diagonally to the left in front of the pedestrian. It is close to the pedestrian and the pedestrian is visible within the vehicle's field of view. The vehicle is currently stopped, with a speed of 0 km/h. The environment conditions surrounding the event include a female pedestrian in her 40s, standing at a height of 160 cm. She is wearing a purplish red T-shirt and a gray skirt. The event is taking place in an urban area on a weekday, with clear weather and bright brightness. The road surface conditions are dry and level, with the road classified as a main road consisting of one-way traffic with only one lane. Both sides of the road have sidewalks.   \n",
              "28                                                                                                                                                                                                                                                                                                                                                                                                             The vehicle in question is currently undergoing acceleration. It is positioned diagonally to the left in front of a pedestrian and is relatively close to them. The vehicle has a clear field of view of the pedestrian and is currently at a stop, with a speed of 0km/h. In terms of the environment condition, the pedestrian is a female in her 40s, standing at approximately 160 cm in height. She is wearing a purplish red T-shirt on her upper body and a gray skirt on her lower body. The event takes place in an urban setting on a weekday, with clear and bright weather conditions. The road surface is dry and level, consisting of asphalt. The traffic volume is usual, and the road is classified as a main road with a single one-way lane. There are sidewalks available on both sides of the road.   \n",
              "29  The vehicle is currently in a state of constant speed, indicating that it is maintaining a steady pace without any acceleration or deceleration. It is positioned on the left side of a pedestrian, implying that the vehicle is located to the left of the individual as viewed from the vehicle's perspective. The relative distance between the vehicle and the pedestrian is close, indicating a short distance between them. From the vehicle's field of view, the pedestrian is visible and can be seen by the driver. The vehicle itself is currently in a stopped position, suggesting that it is not in motion. Its speed is recorded as 0 km/h, indicating complete immobility. As for the environment conditions, the pedestrian is a female in her 40s, who stands at a height of 160 cm. She is wearing a purplish red T-shirt on her upper body and a gray skirt on her lower body. The event takes place in an urban area, specifically on a weekday, with a clear and bright weather condition. The road surface is dry and level, composed of asphalt. The traffic volume on the road is usual, and the road itself is classified as a main road with a single one-way lane and sidewalks available on both sides.   \n",
              "30                                                                                                                                                                                                                                                                                                                                                                                                                                         The vehicle was moving at a constant speed of 20km/h. It was positioned diagonally to the left in front of a female pedestrian who was in her 30s and approximately 170 cm tall. The distance between the vehicle and the pedestrian was far. The vehicle had a clear field of view of the pedestrian. It continued to move straight ahead. The environment conditions indicated that it was a weekday and the weather was snowy with bright lighting. The road surface was frozen, but level with asphalt. The surrounding area was urban with usual traffic volume. The road was a main road with one-way traffic and three lanes. There were sidewalks present on both sides of the road. Additionally, the pedestrian was wearing a dark green hat, dark green jacket, and black slacks.   \n",
              "31                                                                                                                                                                                                                                                                                                                                      The vehicle moves at a constant speed of 20km/h, displaying no signs of acceleration or deceleration. It is positioned diagonally to the left in front of the pedestrian, who is far away. The vehicle's field of view allows it to see the pedestrian clearly. The vehicle continues its straight path, unaffected by any obstacles. Meanwhile, the female pedestrian in her 30s, standing at a height of 170 cm, is wearing a dark green hat, jacket, and black slacks. The setting is an urban environment on a weekday, with bright snowy weather. The road surface is frozen, but the incline remains level. The vehicle is traveling on a main road with one-way traffic and three lanes, while both sides of the road have sidewalks. Overall, the vehicle's motion, position, and actions align with the surrounding environmental conditions, allowing it to navigate the road safely.   \n",
              "32                                                                                                                                                                                                                                                                                                                                                                                                                                                          The vehicle is moving at a constant speed of 20km/h. It is positioned diagonally to the left in front of the pedestrian and is relatively near to them. The vehicle can see the pedestrian within its field of view. It is going straight ahead without any changes in direction. Meanwhile, the environment conditions reveal that the pedestrian is a female in her 30s, standing at a height of 170 cm. She is wearing a dark green hat, jacket, and black slacks. The event is taking place in an urban area on a weekday. The weather is snowy with bright brightness. The road surface is frozen and inclined level on an asphalt road. The traffic volume is usual, and the road is classified as a main road with one-way, three lanes and sidewalks on both sides.   \n",
              "33                                                                                                                                                                                                                                                                                                                                                                                             The vehicle was moving at a constant speed of 20km/h. It was positioned diagonally to the left in front of a pedestrian, who was near to the vehicle. The pedestrian was visible within the vehicle's field of view. The vehicle was going straight ahead and the road surface was frozen. The vehicle was in an urban environment and it was a weekday. The weather was snowy and the brightness was bright. The vehicle was on a main road, which had one-way traffic with three lanes. The road surface was asphalt and the sidewalk was present on both sides. The vehicle was in an environment that had usual traffic volume. The pedestrian in the environment was a female in her 30s, approximately 170 cm tall. She was wearing a dark green hat, a dark green jacket, and black slacks. The event took place on a level road.   \n",
              "34                                                                                                                                                                                                                                                                                                                                                                                              The vehicle is moving at a constant speed of 20km/h. It is positioned diagonally to the left in front of the pedestrian. The vehicle is near to the pedestrian and within its field of view. The vehicle is going straight ahead. Meanwhile, the environment condition reveals that the pedestrian is a female in her 30s with a height of 170cm. She is wearing a dark green hat, jacket, and black slacks. The surroundings are urban, and it is a weekday with snowy weather. The brightness is bright, and the road surface conditions are frozen. The road is a main road with one-way, three-lane traffic, and there are sidewalks on both sides. These details present a scenario where a vehicle is likely to encounter a pedestrian crossing the snow-covered frozen road in an urban environment on a bright weekday morning.   \n",
              "35                                                                                                                                                                                                                                                            The vehicle is on the left side of the pedestrian and is close to them. The pedestrian is visible to the vehicle and it takes emergency action by braking to avoid a collision. The vehicle is stationary, with a speed of 0 km/h. The environment conditions reveal that the pedestrian is a male in his 20s, approximately 170 cm tall. He is wearing a black T-shirt and black slacks. The weather is cloudy, with low brightness and dry road surface conditions. The road is level and made of asphalt. The traffic volume is light on this residential road, which has two-way traffic. The event takes place at an intersection with signal lights. There is no sidewalk on both sides and no roadside strip. However, there are street lights illuminating the area. Overall, the vehicle maintains a safe distance from the pedestrian and takes prompt action to avoid any potential danger, considering the environmental conditions and road characteristics.   \n",
              "36                                                                                                                                                                                                                           The vehicle is positioned on the left side of the pedestrian, and it is close in relative distance. The vehicle's field of view indicates that the pedestrian is visible. The vehicle is currently turning right at a speed of 20 km/h. In terms of the environment conditions, the pedestrian is a male in his 20s, with a height of 170 cm. He is wearing a black T-shirt for the upper body and black slacks for the lower body. The weather is cloudy and the brightness level is dark. The road surface conditions are dry and level, with asphalt as the road surface type. The traffic volume is light and the road classification is a residential road with two-way traffic. The road form is an intersection with a signal. There is no sidewalk or roadside strip on both sides, but street lights are present. With this information, it can be described that the vehicle is currently in the process of making a right turn while being aware of the pedestrian's presence on the left side.   \n",
              "37                                                                                                                                                                                                                                                                                                                                                                                                                           The vehicle, traveling at a speed of 20 km/h, was positioned behind and to the left of the pedestrian. The vehicle was at a close distance to the pedestrian and had a clear view of them. It started to turn right. The vehicle and the pedestrian were on a residential road, which had two-way traffic. The road was dry, level, and made of asphalt. The vehicle and pedestrian were at an intersection with a traffic signal in place. The environment conditions indicated that the pedestrian was a male in his 20s, with a height of 170 cm. He was wearing a black T-shirt and black slacks. The weather was cloudy, and it was dark outside. The road had light traffic volume and was not illuminated by street lights. There was no sidewalk and no roadside strip on either side of the road.   \n",
              "38                                                                                                                                                                                                                                                                                                    The vehicle is positioned behind on the left of the pedestrian and is relatively near to them. The pedestrian is within the vehicle's field of view, indicating visibility. The vehicle is moving straight ahead at a speed of 20 km/h. The road surface conditions are dry, and the road is level. The vehicle is traveling on a two-way traffic residential road, approaching an intersection with a signal. The environment conditions indicate that the pedestrian is a male in his 20s, standing at a height of 170 cm. He is wearing a black T-shirt on the upper body and black slacks on the lower body. The weather is cloudy, and the brightness is dark. The road surface is asphalt, and the traffic volume is light. There are no sidewalks on both sides of the road, and only one side has a roadside strip. Street lights are present. This information describes the scenario in which the vehicle is operating.   \n",
              "39                                                                                                                                                                                                                                                                                                                                                                          The vehicle, traveling at a speed of 20 km/h, is positioned behind to the left of a pedestrian. The relative distance between the vehicle and the pedestrian is near. From the vehicle's field of view, the pedestrian is visible. The vehicle is currently going straight ahead. The environment conditions surrounding the vehicle and the pedestrian are as follows: The pedestrian is a male in his 20s, with a height of 170 cm. He is wearing a black T-shirt and black slacks. The weather is cloudy, and the brightness level is dark. The road surface conditions are dry and level, with asphalt as the road surface type. The traffic volume is light on this residential road, which has two-way traffic and an intersection with a signal. There are no sidewalks or roadside strips on both sides of the road, but street lights are present.   \n",
              "\n",
              "    start_time  end_time  \n",
              "20      20.954    21.021  \n",
              "21      21.054    21.121  \n",
              "22      21.154    21.288  \n",
              "23      21.321    23.357  \n",
              "24      23.390    24.892  \n",
              "25      19.633    19.967  \n",
              "26      20.000    22.267  \n",
              "27      22.300    22.367  \n",
              "28      22.400    24.167  \n",
              "29      24.200    27.967  \n",
              "30      37.871    38.172  \n",
              "31      38.205    38.438  \n",
              "32      38.472    38.705  \n",
              "33      38.739    39.139  \n",
              "34      39.173    39.973  \n",
              "35      31.438    34.229  \n",
              "36      30.415    31.436  \n",
              "37      29.418    30.412  \n",
              "38      28.419    29.415  \n",
              "39      27.411    28.419  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-483d8c45-61c3-46b9-b6c8-48a5c05429c6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>video_name</th>\n",
              "      <th>fps</th>\n",
              "      <th>label</th>\n",
              "      <th>caption_pedestrian</th>\n",
              "      <th>caption_vehicle</th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>9fa4da9c-4b60-48d9-a159-53547b02aedf</td>\n",
              "      <td>video3187.mp4</td>\n",
              "      <td>29.97</td>\n",
              "      <td>prerecognition</td>\n",
              "      <td>The pedestrian, a middle-aged male in his 50s, stood in an urban setting on a bright weekday morning. He was wearing a blue T-shirt and slacks, his height measuring around 170 cm. With glasses perched on his nose and a black hat atop his head, he appeared to be closely watching his surroundings. Unaware of the vehicle directly in front of him, the pedestrian's body was positioned perpendicular to the vehicle and to the right. His line of sight was fixed straight ahead, aligned with the direction of travel. The pedestrian seemed to be in no rush, moving slowly as he attempted to cross the road. The conditions were optimal, with the road surface dry and the weather clear. Despite the light traffic volume on the main road, there was only one lane available for vehicles traveling in one direction. Sidewalks were present on both sides, adding to the pedestrian's sense of safety and ease as he made his way across.</td>\n",
              "      <td>The vehicle was moving at a constant speed when it suddenly came to a stop. Positioned on the right side of a pedestrian, the vehicle was in close proximity to them. The pedestrian was clearly visible within the vehicle's field of view. The vehicle's speed was at 0km/h, indicating a complete halt. In terms of the environment conditions, the pedestrian was a male in his 50s, standing at a height of 170 cm. He was wearing glasses, a black hat, a blue T-shirt, and blue slacks. The event took place in an urban area on a weekday with clear weather and bright brightness. The road surface was dry and level, consisting of asphalt. The traffic volume was light as the vehicle was traveling on a main road with a single one-way lane and sidewalks on both sides.</td>\n",
              "      <td>20.954</td>\n",
              "      <td>21.021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>9fa4da9c-4b60-48d9-a159-53547b02aedf</td>\n",
              "      <td>video3187.mp4</td>\n",
              "      <td>29.97</td>\n",
              "      <td>recognition</td>\n",
              "      <td>The pedestrian is a middle-aged man in his 50s with a height of 170 cm. He is wearing a black hat, a blue T-shirt as an upper body clothing, and blue slacks as lower body clothing. He is also wearing glasses. The event takes place in an urban area on a clear weekday. The weather is bright and the road conditions are dry. The road surface is made of asphalt and there is only one lane on the one-way main road. There is a sidewalk on both sides. The pedestrian is standing directly in front of a vehicle while facing perpendicular to it and to the right. He is closely watching the vehicle and is almost aware of its presence. His line of sight is in front, aligning with the vehicle's direction of travel. He is slowly crossing the road and appears to be cautious. The traffic volume is light and the road is level.</td>\n",
              "      <td>The vehicle is currently in a state of constant speed and is positioned on the right side of a pedestrian. They are situated in close proximity to each other, with the vehicle being able to see the pedestrian within its field of view. At this moment, the vehicle has come to a complete stop, with its speed at 0 km/h. As for the environmental conditions, the pedestrian is a male in his 50s, standing at a height of 170 cm. The pedestrian is wearing glasses and a black hat, along with a blue t-shirt and blue slacks. The event takes place in an urban area on a weekday. The weather is clear, and the brightness is bright. The road conditions are optimal, with the asphalt road surface being dry and level. This particular location is classified as a main road with a single one-way lane and sidewalks on both sides. The traffic volume is light, allowing for a relatively calm atmosphere.</td>\n",
              "      <td>21.054</td>\n",
              "      <td>21.121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>9fa4da9c-4b60-48d9-a159-53547b02aedf</td>\n",
              "      <td>video3187.mp4</td>\n",
              "      <td>29.97</td>\n",
              "      <td>judgement</td>\n",
              "      <td>The pedestrian, a man in his 50s wearing a blue T-shirt and slacks, with a black hat and glasses, stands directly in front of the vehicle on the urban main road on a clear and bright weekday. His body is positioned perpendicular to the vehicle and to the right. He is close to the vehicle, and his line of sight is in front, in the direction of travel. The pedestrian closely watches his surroundings, almost noticing the presence of the vehicle. He is aware of the vehicle and slowly crosses the road. The road surface is dry and level, with light traffic volume. There is one-way traffic on the road, with a single lane and sidewalks on both sides. The environment presents a calm and steady atmosphere as the pedestrian cautiously proceeds with his crossing, taking into account the surroundings and his own visual state. Overall, the situation remains under control and harmonious, with no immediate danger or rush.</td>\n",
              "      <td>The vehicle is currently stationary, maintaining a constant speed, with a close relative distance to a pedestrian on its right side. The vehicle's field of view allows it to see the pedestrian clearly. Analyzing the surroundings, it can be observed that the weather is clear, the brightness is bright, and the road surface conditions are dry. The vehicle is situated in an urban area, specifically on a main road with a single lane in the direction it is traveling. There are sidewalks on both sides of the road. The environment conditions indicate that the pedestrian is a male in his 50s, approximately 170 cm tall, wearing glasses and a black hat. His upper body is adorned with a blue T-shirt, and he is wearing blue slacks for his lower body. This event occurs on a weekday with light traffic volume on the road. The vehicle is placed on level asphalt.</td>\n",
              "      <td>21.154</td>\n",
              "      <td>21.288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>9fa4da9c-4b60-48d9-a159-53547b02aedf</td>\n",
              "      <td>video3187.mp4</td>\n",
              "      <td>29.97</td>\n",
              "      <td>action</td>\n",
              "      <td>The pedestrian stands perpendicular to the vehicle and to the right. Positioned directly in front of the vehicle, the pedestrian is relatively close to it. Their line of sight is in front, aligned with the direction of travel. Closely watching, the pedestrian's visual status indicates their awareness of the surroundings. Moving slowly, the pedestrian is making their way across. In terms of the environment condition, the pedestrian is a male in his 50s, with a height of 170 cm. He wears glasses and a black hat, and his upper body is adorned with a blue t-shirt. His lower body is clad in blue slacks. Taking place in an urban setting on a clear and bright weekday, the road surface is dry and level, made of asphalt. The traffic volume is light, and it is a one-way, one-lane road with sidewalks on both sides. The event unfolds on a main road.</td>\n",
              "      <td>The vehicle is currently experiencing constant speed as it moves along. Positioned on the right side of the pedestrian, it is close in relative distance. From its field of view, the pedestrian is visible to the vehicle. The vehicle is in a stopped state with a speed of 0km/h. In the surrounding environment, a male in his 50s is present. He has a height of 170 cm and wears glasses. Additionally, he sports a black hat, a blue T-shirt for his upper body, and blue slacks for his lower body. The setting of this event takes place in an urban area, specifically on a weekday. The weather is clear with bright lighting conditions. The road surface is dry and level, made of asphalt. It is a main road with one-way traffic and a single lane. Sidewalks are available on both sides. The traffic volume is light in this scenario.</td>\n",
              "      <td>21.321</td>\n",
              "      <td>23.357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>9fa4da9c-4b60-48d9-a159-53547b02aedf</td>\n",
              "      <td>video3187.mp4</td>\n",
              "      <td>29.97</td>\n",
              "      <td>avoidance</td>\n",
              "      <td>The pedestrian, a middle-aged man in his 50s, was standing directly in front of the vehicle on a clear weekday. He was relatively close to the vehicle, with his body perpendicular and to the right. Closely watching, he had a line of sight in front, following the direction of the vehicle's travel. Although he had almost noticed the vehicle's presence, he was slowly crossing the road. The man's appearance included glasses and he was dressed in a blue T-shirt and slacks, with a black hat. The urban environment offered bright lighting, with dry and level asphalt road conditions. The main road had light traffic with only one lane, and sidewalks were present on both sides. These details painted a clear picture of the pedestrian's situation, providing important information on his relative position to the vehicle and overall surroundings.</td>\n",
              "      <td>The vehicle is currently experiencing acceleration and is positioned on the right side of a pedestrian. The vehicle is at a close distance from the pedestrian and has a clear field of view where it can see the pedestrian. The vehicle is about to start moving and is moving at a speed of 5km/h. The environment conditions show that the pedestrian is a male in his 50s, with a height of 170 cm. He is wearing glasses and a black hat, along with a blue T-shirt and blue slacks. The event is taking place in an urban setting, specifically on a weekday. The weather is clear and bright, with dry road surface conditions on a level asphalt road. The traffic volume is light and the road is classified as a main road with one-way traffic and one lane. There are sidewalks present on both sides of the road.</td>\n",
              "      <td>23.390</td>\n",
              "      <td>24.892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>99934c50-d5d3-4c44-a9e3-9450fe8e1ceb</td>\n",
              "      <td>video1248.mp4</td>\n",
              "      <td>30.00</td>\n",
              "      <td>prerecognition</td>\n",
              "      <td>The pedestrian, a woman in her 40s, stands still on a clear, bright weekday morning. She is wearing a purplish red T-shirt and a gray skirt, her height measuring approximately 160 cm. Oblivious to the vehicle nearby, she closely watches her intended crossing destination. The pedestrian's body is oriented perpendicularly to the left side of the vehicle while positioning herself diagonally to the right and in front of it. In this urban environment, on a level, dry asphalt road with usual traffic volume, the main road consists of a one-way lane with sidewalks on both sides. The pedestrian seems unaware of the vehicle's presence, as she remains engrossed in her surroundings. Despite the busy traffic and the potential danger it poses, she stands still, perhaps waiting for the opportune moment to move across the road.</td>\n",
              "      <td>A vehicle is decelerating while travelling diagonally to the left in front of a pedestrian. The vehicle is near the pedestrian and has a clear field of view with the pedestrian visible. It is moving straight ahead at a speed of 10km/h. The vehicle is in an urban environment and it is a weekday. The weather is clear and the brightness is bright. The road surface conditions are dry and level, with the road surface being asphalt. The traffic volume is normal and the road is classified as a main road with one way and one lane. There are sidewalks present on both sides of the road. As for the environment condition, the pedestrian is a female in her 40s with a height of 160 cm. She is wearing a purplish red T-shirt and a gray skirt.</td>\n",
              "      <td>19.633</td>\n",
              "      <td>19.967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>99934c50-d5d3-4c44-a9e3-9450fe8e1ceb</td>\n",
              "      <td>video1248.mp4</td>\n",
              "      <td>30.00</td>\n",
              "      <td>recognition</td>\n",
              "      <td>A female pedestrian in her 40s with a height of 160 cm was standing still on a main road. She was wearing a purplish red T-shirt and a gray skirt. The pedestrian's body was perpendicular to the vehicle and positioned diagonally to the right, in front of the vehicle. She had a close line of sight to her crossing destination and was closely watching her surroundings. Although she was almost noticed, she was aware of the vehicle. The pedestrian was in an urban area on a weekday, with clear weather and bright lighting. The road surface conditions were dry and level, with asphalt as the road surface type. The traffic volume was usual on this one-way, one-lane road, which had sidewalks on both sides.</td>\n",
              "      <td>The vehicle is currently decelerating and is positioned diagonally to the left in front of the pedestrian. The vehicle is near to the pedestrian and has a clear view of them. It appears that the vehicle is about to stop. Additionally, the vehicle is traveling at a speed of 5km/h. As for the environmental conditions, the pedestrian is a female in her 40s, standing at a height of 160 cm. She is wearing a purplish red T-shirt and a gray skirt. The event takes place in an urban setting on a clear and bright weekday. The road surface is dry and level, with asphalt as the type of road surface. The traffic volume is usual, and the road is classified as a main road with one-way traffic flow and one lane. Sidewalks are available on both sides of the road.</td>\n",
              "      <td>20.000</td>\n",
              "      <td>22.267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>99934c50-d5d3-4c44-a9e3-9450fe8e1ceb</td>\n",
              "      <td>video1248.mp4</td>\n",
              "      <td>30.00</td>\n",
              "      <td>judgement</td>\n",
              "      <td>The pedestrian, a female in her 40s, stood still diagonally to the right in front of the vehicle on a clear and bright weekday. She was wearing a purplish red T-shirt and a gray skirt, standing on the dry asphalt road surface. The road was a main road with only one way and one lane, and there were sidewalks on both sides. The pedestrian's body was oriented diagonally to the left, opposite to the direction of the vehicle. She closely watched the road surface, indicating her visual focus. Although she had almost noticed the vehicle, she was unaware of its presence. The relative distance between the pedestrian and the vehicle was close. The overall environment conditions were urban, and the traffic volume was usual. The pedestrian's line of sight was unobstructed, and the road surface was level. These details provide a clear understanding of the situation surrounding the pedestrian and the environment they were in at that particular time.</td>\n",
              "      <td>The vehicle is positioned diagonally to the left in front of the pedestrian. It is close to the pedestrian and the pedestrian is visible within the vehicle's field of view. The vehicle is currently stopped, with a speed of 0 km/h. The environment conditions surrounding the event include a female pedestrian in her 40s, standing at a height of 160 cm. She is wearing a purplish red T-shirt and a gray skirt. The event is taking place in an urban area on a weekday, with clear weather and bright brightness. The road surface conditions are dry and level, with the road classified as a main road consisting of one-way traffic with only one lane. Both sides of the road have sidewalks.</td>\n",
              "      <td>22.300</td>\n",
              "      <td>22.367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>99934c50-d5d3-4c44-a9e3-9450fe8e1ceb</td>\n",
              "      <td>video1248.mp4</td>\n",
              "      <td>30.00</td>\n",
              "      <td>action</td>\n",
              "      <td>The pedestrian, a woman in her 40s, stood diagonally to the right and in front of the vehicle. She closely watched the vehicle, almost noticed its presence. Her body was oriented diagonally to the left, opposite to the direction of the vehicle. With her line of sight fixed on her crossing destination, she appeared ready to cross. Moving slowly, she stood in front of the vehicle, seemingly aware of its presence. The woman's clothing consisted of a purplish red T-shirt on her upper body and a gray skirt on her lower body. The urban environment was characterized by a clear and bright weather on a dry asphalt road, with usual traffic volume. The road was a main road with only one lane in the direction the pedestrian was facing. Sidewalks were present on both sides.</td>\n",
              "      <td>The vehicle in question is currently undergoing acceleration. It is positioned diagonally to the left in front of a pedestrian and is relatively close to them. The vehicle has a clear field of view of the pedestrian and is currently at a stop, with a speed of 0km/h. In terms of the environment condition, the pedestrian is a female in her 40s, standing at approximately 160 cm in height. She is wearing a purplish red T-shirt on her upper body and a gray skirt on her lower body. The event takes place in an urban setting on a weekday, with clear and bright weather conditions. The road surface is dry and level, consisting of asphalt. The traffic volume is usual, and the road is classified as a main road with a single one-way lane. There are sidewalks available on both sides of the road.</td>\n",
              "      <td>22.400</td>\n",
              "      <td>24.167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>99934c50-d5d3-4c44-a9e3-9450fe8e1ceb</td>\n",
              "      <td>video1248.mp4</td>\n",
              "      <td>30.00</td>\n",
              "      <td>avoidance</td>\n",
              "      <td>The pedestrian stood perpendicular to the vehicle and to the left. They were positioned directly in front of the vehicle, at a close relative distance. Their line of sight was focused on their crossing destination, and they closely watched the surroundings. Moving slowly, they were heading across the road. The pedestrian, an adult female in her 40s, stood at a height of 160 cm. She was wearing a purplish red T-shirt, and a gray skirt. The event took place in an urban area on a clear weekday with bright lighting conditions. The road surface was dry and level, made of asphalt. It was a main road with one-way traffic and one lane, with sidewalks available on both sides. The traffic volume was normal, and the surroundings were typically suburban. This information provides a comprehensive description of the pedestrian's orientation, appearance, location, and the environmental conditions in which the event occurred.</td>\n",
              "      <td>The vehicle is currently in a state of constant speed, indicating that it is maintaining a steady pace without any acceleration or deceleration. It is positioned on the left side of a pedestrian, implying that the vehicle is located to the left of the individual as viewed from the vehicle's perspective. The relative distance between the vehicle and the pedestrian is close, indicating a short distance between them. From the vehicle's field of view, the pedestrian is visible and can be seen by the driver. The vehicle itself is currently in a stopped position, suggesting that it is not in motion. Its speed is recorded as 0 km/h, indicating complete immobility. As for the environment conditions, the pedestrian is a female in her 40s, who stands at a height of 160 cm. She is wearing a purplish red T-shirt on her upper body and a gray skirt on her lower body. The event takes place in an urban area, specifically on a weekday, with a clear and bright weather condition. The road surface is dry and level, composed of asphalt. The traffic volume on the road is usual, and the road itself is classified as a main road with a single one-way lane and sidewalks available on both sides.</td>\n",
              "      <td>24.200</td>\n",
              "      <td>27.967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>79a3074c-651f-4726-882c-a64c38d42b6b</td>\n",
              "      <td>video157.mp4</td>\n",
              "      <td>29.97</td>\n",
              "      <td>prerecognition</td>\n",
              "      <td>The pedestrian is a female in her 30s, standing still on a main road in an urban area. She is wearing a dark green jacket, slacks, and a dark green hat. The weather is snowy, with bright brightness. The road surface is frozen, but there is no incline. The road is an asphalt surface with one-way traffic and three lanes. The pedestrian's body is oriented diagonally to the left, which is opposite to the direction of the vehicle. She is positioned diagonally to the right, in front of the vehicle. The pedestrian's line of sight is in front, following the direction of travel. She is closely watching her surroundings. However, she is unaware of the vehicle approaching her. Both sides of the road have sidewalks. This event occurs on a weekday and the traffic volume is usual.</td>\n",
              "      <td>The vehicle was moving at a constant speed of 20km/h. It was positioned diagonally to the left in front of a female pedestrian who was in her 30s and approximately 170 cm tall. The distance between the vehicle and the pedestrian was far. The vehicle had a clear field of view of the pedestrian. It continued to move straight ahead. The environment conditions indicated that it was a weekday and the weather was snowy with bright lighting. The road surface was frozen, but level with asphalt. The surrounding area was urban with usual traffic volume. The road was a main road with one-way traffic and three lanes. There were sidewalks present on both sides of the road. Additionally, the pedestrian was wearing a dark green hat, dark green jacket, and black slacks.</td>\n",
              "      <td>37.871</td>\n",
              "      <td>38.172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>79a3074c-651f-4726-882c-a64c38d42b6b</td>\n",
              "      <td>video157.mp4</td>\n",
              "      <td>29.97</td>\n",
              "      <td>recognition</td>\n",
              "      <td>The pedestrian, a female in her 30s wearing a dark green jacket and slacks, stood still diagonally to the right in front of the vehicle on a bright snowy weekday morning. She was closely watching and noticed the vehicle approaching from the opposite direction diagonally to the left. Her line of sight was in front, in the direction of travel. The pedestrian's body was oriented diagonally to the left, opposite to the vehicle's direction. The road surface was frozen and inclined on the main road, which had one-way, three-lane traffic. The urban environment had sidewalks on both sides. Despite the snowy weather, the brightness was high, providing good visibility. This event occurred in an urban setting with usual traffic volume and an asphalt road surface. The pedestrian's clothing also included a dark green hat, matching the upper body's clothing color. Overall, the pedestrian's actions, visual status, and awareness of the vehicle were observed and described in detail.</td>\n",
              "      <td>The vehicle moves at a constant speed of 20km/h, displaying no signs of acceleration or deceleration. It is positioned diagonally to the left in front of the pedestrian, who is far away. The vehicle's field of view allows it to see the pedestrian clearly. The vehicle continues its straight path, unaffected by any obstacles. Meanwhile, the female pedestrian in her 30s, standing at a height of 170 cm, is wearing a dark green hat, jacket, and black slacks. The setting is an urban environment on a weekday, with bright snowy weather. The road surface is frozen, but the incline remains level. The vehicle is traveling on a main road with one-way traffic and three lanes, while both sides of the road have sidewalks. Overall, the vehicle's motion, position, and actions align with the surrounding environmental conditions, allowing it to navigate the road safely.</td>\n",
              "      <td>38.205</td>\n",
              "      <td>38.438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>79a3074c-651f-4726-882c-a64c38d42b6b</td>\n",
              "      <td>video157.mp4</td>\n",
              "      <td>29.97</td>\n",
              "      <td>judgement</td>\n",
              "      <td>A pedestrian, a woman in her 30s, stands still diagonally to the right, in front of the vehicle. She is oriented diagonally to the left, opposite to the vehicle's direction. Her line of sight is in front, following the vehicle's path. Closely watching, she notices the vehicle and is aware of its presence. The pedestrian's clothing consists of a dark green jacket, dark green hat, and black slacks. The environment is urban and it is a weekday. Despite the bright snowfall, the visibility is clear. The road surface conditions are frozen on a level asphalt road. The traffic volume is usual on this one-way main road with three lanes, and there are sidewalks on both sides.</td>\n",
              "      <td>The vehicle is moving at a constant speed of 20km/h. It is positioned diagonally to the left in front of the pedestrian and is relatively near to them. The vehicle can see the pedestrian within its field of view. It is going straight ahead without any changes in direction. Meanwhile, the environment conditions reveal that the pedestrian is a female in her 30s, standing at a height of 170 cm. She is wearing a dark green hat, jacket, and black slacks. The event is taking place in an urban area on a weekday. The weather is snowy with bright brightness. The road surface is frozen and inclined level on an asphalt road. The traffic volume is usual, and the road is classified as a main road with one-way, three lanes and sidewalks on both sides.</td>\n",
              "      <td>38.472</td>\n",
              "      <td>38.705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>79a3074c-651f-4726-882c-a64c38d42b6b</td>\n",
              "      <td>video157.mp4</td>\n",
              "      <td>29.97</td>\n",
              "      <td>action</td>\n",
              "      <td>The pedestrian, a female in her 30s, stood still on the frozen asphalt road of the urban environment. Her body was oriented diagonally to the left, opposite to the direction of an approaching vehicle. Positioned diagonally to the right in front of the vehicle, she closely watched its movements. Despite the bright snowy weather, her line of sight was focused in the direction of travel. The pedestrian, dressed in a dark green jacket and slacks, also wore a dark green hat. The road, a main one-way thoroughfare with three lanes, had sidewalks on both sides. It was a weekday, and traffic volume was usual. The surroundings were illuminated, and the road surface conditions were icy. Amidst these circumstances, the pedestrian, standing still, noticed the vehicle but did not seem to display any immediate or specific actions.</td>\n",
              "      <td>The vehicle was moving at a constant speed of 20km/h. It was positioned diagonally to the left in front of a pedestrian, who was near to the vehicle. The pedestrian was visible within the vehicle's field of view. The vehicle was going straight ahead and the road surface was frozen. The vehicle was in an urban environment and it was a weekday. The weather was snowy and the brightness was bright. The vehicle was on a main road, which had one-way traffic with three lanes. The road surface was asphalt and the sidewalk was present on both sides. The vehicle was in an environment that had usual traffic volume. The pedestrian in the environment was a female in her 30s, approximately 170 cm tall. She was wearing a dark green hat, a dark green jacket, and black slacks. The event took place on a level road.</td>\n",
              "      <td>38.739</td>\n",
              "      <td>39.139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>79a3074c-651f-4726-882c-a64c38d42b6b</td>\n",
              "      <td>video157.mp4</td>\n",
              "      <td>29.97</td>\n",
              "      <td>avoidance</td>\n",
              "      <td>The pedestrian consists of a female in her 30s, with a height of 170 cm. She is dressed in a dark green jacket and slacks, wearing a dark green hat. Currently, she is located diagonally to the right and in front of a vehicle, standing still. Despite the snowy weather with bright lighting, the pedestrian closely watches the vehicle, as she notices its presence. Her line of sight is directed in front, aligned with the vehicle's direction of travel. The road conditions are frozen, and the road surface type is asphalt. The environment in which this event takes place is urban, with an average traffic volume on a main road consisting of one-way access and three lanes. Additionally, there are sidewalks available on both sides. The event occurs on a weekday, and the pedestrian exhibits an orientation of her body, diagonally to the left, opposing the vehicle's direction.</td>\n",
              "      <td>The vehicle is moving at a constant speed of 20km/h. It is positioned diagonally to the left in front of the pedestrian. The vehicle is near to the pedestrian and within its field of view. The vehicle is going straight ahead. Meanwhile, the environment condition reveals that the pedestrian is a female in her 30s with a height of 170cm. She is wearing a dark green hat, jacket, and black slacks. The surroundings are urban, and it is a weekday with snowy weather. The brightness is bright, and the road surface conditions are frozen. The road is a main road with one-way, three-lane traffic, and there are sidewalks on both sides. These details present a scenario where a vehicle is likely to encounter a pedestrian crossing the snow-covered frozen road in an urban environment on a bright weekday morning.</td>\n",
              "      <td>39.173</td>\n",
              "      <td>39.973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1663</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>The pedestrian, a young man in his twenties, stands diagonally to the right of the vehicle, completely unaware of its presence. He is positioned directly in front of the car, close in distance. His body is motionless and his line of sight is immediately above. The pedestrian's attire consists of a black T-shirt and black slacks, matching the dark and cloudy weather. The road conditions are favorable, with the dry asphalt providing a level surface. The residential road intersects with a signal, indicating that both the vehicle and the pedestrian should exercise caution. Although the traffic volume is light, there are two-way lanes available. On this particular street, there are no sidewalks or roadside strips on both sides, but there are street lights illuminating the path. This simple event captures a snapshot of the pedestrian's surroundings and his lack of awareness towards the vehicle approaching him.</td>\n",
              "      <td>The vehicle is on the left side of the pedestrian and is close to them. The pedestrian is visible to the vehicle and it takes emergency action by braking to avoid a collision. The vehicle is stationary, with a speed of 0 km/h. The environment conditions reveal that the pedestrian is a male in his 20s, approximately 170 cm tall. He is wearing a black T-shirt and black slacks. The weather is cloudy, with low brightness and dry road surface conditions. The road is level and made of asphalt. The traffic volume is light on this residential road, which has two-way traffic. The event takes place at an intersection with signal lights. There is no sidewalk on both sides and no roadside strip. However, there are street lights illuminating the area. Overall, the vehicle maintains a safe distance from the pedestrian and takes prompt action to avoid any potential danger, considering the environmental conditions and road characteristics.</td>\n",
              "      <td>31.438</td>\n",
              "      <td>34.229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1663</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>The pedestrian, a male in his 20s with a height of 170 cm, was standing still on a residential road intersection with signal. He was diagonally positioned to the right, in the same direction as the vehicle, and directly in front of it. Unaware of the vehicle's presence, his line of sight was immediately above. The pedestrian's body language showed no signs of abnormal activity, as he was calmly standing still. He was wearing a black T-shirt on his upper body and black slacks on his lower body. The weather was cloudy, resulting in a dark brightness. The road surface was dry and level, made of asphalt. The traffic volume was light on the two-way traffic residential road. There were no sidewalks on both sides, and neither were there roadside strips. However, street lights were available, providing illumination. The overall conditions of the environment indicate that the pedestrian was stationary in a potentially unsafe position, with limited visibility due to the darkness and cloudy weather.</td>\n",
              "      <td>The vehicle is positioned on the left side of the pedestrian, and it is close in relative distance. The vehicle's field of view indicates that the pedestrian is visible. The vehicle is currently turning right at a speed of 20 km/h. In terms of the environment conditions, the pedestrian is a male in his 20s, with a height of 170 cm. He is wearing a black T-shirt for the upper body and black slacks for the lower body. The weather is cloudy and the brightness level is dark. The road surface conditions are dry and level, with asphalt as the road surface type. The traffic volume is light and the road classification is a residential road with two-way traffic. The road form is an intersection with a signal. There is no sidewalk or roadside strip on both sides, but street lights are present. With this information, it can be described that the vehicle is currently in the process of making a right turn while being aware of the pedestrian's presence on the left side.</td>\n",
              "      <td>30.415</td>\n",
              "      <td>31.436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1663</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>The pedestrian is a male in his 20s, approximately 170 cm tall, wearing a black T-shirt and black slacks. He is standing diagonally to the right, in front of the vehicle, with his body oriented diagonally to the right as well, in the same direction as the vehicle. The pedestrian is close to the vehicle, and his line of sight is immediately above. He is unaware of the vehicle's presence. The overall environment is cloudy and dark, with a dry asphalt road surface. The road is a residential road with two-way traffic and an intersection with a signal. There are no sidewalks on both sides, and there are no roadside strips; however, there are street lights present. The pedestrian's general action is to stand still, but his abnormal action is lying stretched out. The traffic volume is light.</td>\n",
              "      <td>The vehicle, traveling at a speed of 20 km/h, was positioned behind and to the left of the pedestrian. The vehicle was at a close distance to the pedestrian and had a clear view of them. It started to turn right. The vehicle and the pedestrian were on a residential road, which had two-way traffic. The road was dry, level, and made of asphalt. The vehicle and pedestrian were at an intersection with a traffic signal in place. The environment conditions indicated that the pedestrian was a male in his 20s, with a height of 170 cm. He was wearing a black T-shirt and black slacks. The weather was cloudy, and it was dark outside. The road had light traffic volume and was not illuminated by street lights. There was no sidewalk and no roadside strip on either side of the road.</td>\n",
              "      <td>29.418</td>\n",
              "      <td>30.412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1663</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>In a residential road intersection on a cloudy day, a male pedestrian in his 20s with a height of 170 cm stands diagonally to the right, in front of a vehicle. The pedestrian is wearing a black T-shirt and black slacks. With his line of sight immediately above, he remains unaware of the vehicle's presence, which is positioned near him. The road conditions are dry and level with a light traffic volume. The road surface is asphalt, and there are two lanes for two-way traffic. Although the surroundings are dark, street lights illuminate the area. The pedestrian is standing still, not moving or taking any abnormal actions. As far as the environment is concerned, there are no sidewalks on both sides, nor are there roadside strips on both sides. Despite the environmental factors and proximity to the vehicle, the pedestrian appears to be uninformed about its existence and continues to remain stationary.</td>\n",
              "      <td>The vehicle is positioned behind on the left of the pedestrian and is relatively near to them. The pedestrian is within the vehicle's field of view, indicating visibility. The vehicle is moving straight ahead at a speed of 20 km/h. The road surface conditions are dry, and the road is level. The vehicle is traveling on a two-way traffic residential road, approaching an intersection with a signal. The environment conditions indicate that the pedestrian is a male in his 20s, standing at a height of 170 cm. He is wearing a black T-shirt on the upper body and black slacks on the lower body. The weather is cloudy, and the brightness is dark. The road surface is asphalt, and the traffic volume is light. There are no sidewalks on both sides of the road, and only one side has a roadside strip. Street lights are present. This information describes the scenario in which the vehicle is operating.</td>\n",
              "      <td>28.419</td>\n",
              "      <td>29.415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1663</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>The pedestrian, a male in his 20s with a height of 170 cm, was wearing a black T-shirt and black slacks. It was a cloudy and dark day, with the road being dry and the traffic volume light. He was standing diagonally to the right, in front of a vehicle on a residential road intersection with a signal. The pedestrian's body orientation was the same direction as the vehicle, and his line of sight was immediately above. He was closely watching his surroundings, but unaware of the vehicle approaching him. Despite the pedestrian being near the vehicle, the pedestrian was standing still. However, an abnormal action was noticed as he was lying stretched out. The environment condition portrayed a typical urban setting, with asphalt as the road surface type and a two-way traffic road with not both sides having a sidewalk and roadside strip. Street lights were present in the surroundings.</td>\n",
              "      <td>The vehicle, traveling at a speed of 20 km/h, is positioned behind to the left of a pedestrian. The relative distance between the vehicle and the pedestrian is near. From the vehicle's field of view, the pedestrian is visible. The vehicle is currently going straight ahead. The environment conditions surrounding the vehicle and the pedestrian are as follows: The pedestrian is a male in his 20s, with a height of 170 cm. He is wearing a black T-shirt and black slacks. The weather is cloudy, and the brightness level is dark. The road surface conditions are dry and level, with asphalt as the road surface type. The traffic volume is light on this residential road, which has two-way traffic and an intersection with a signal. There are no sidewalks or roadside strips on both sides of the road, but street lights are present.</td>\n",
              "      <td>27.411</td>\n",
              "      <td>28.419</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-483d8c45-61c3-46b9-b6c8-48a5c05429c6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-483d8c45-61c3-46b9-b6c8-48a5c05429c6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-483d8c45-61c3-46b9-b6c8-48a5c05429c6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-50a8f97b-7e6a-4858-89f0-d8b8d8ca2431\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-50a8f97b-7e6a-4858-89f0-d8b8d8ca2431')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-50a8f97b-7e6a-4858-89f0-d8b8d8ca2431 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[20:40]\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"99934c50-d5d3-4c44-a9e3-9450fe8e1ceb\",\n          \"1663\",\n          \"9fa4da9c-4b60-48d9-a159-53547b02aedf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"video3187.mp4\",\n          \"video1248.mp4\",\n          \"video157.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.014638501094228552,\n        \"min\": 29.97,\n        \"max\": 30.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          30.0,\n          29.97\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"1\",\n          \"recognition\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption_pedestrian\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"The pedestrian, a middle-aged male in his 50s, stood in an urban setting on a bright weekday morning. He was wearing a blue T-shirt and slacks, his height measuring around 170 cm. With glasses perched on his nose and a black hat atop his head, he appeared to be closely watching his surroundings. Unaware of the vehicle directly in front of him, the pedestrian's body was positioned perpendicular to the vehicle and to the right. His line of sight was fixed straight ahead, aligned with the direction of travel. The pedestrian seemed to be in no rush, moving slowly as he attempted to cross the road. The conditions were optimal, with the road surface dry and the weather clear. Despite the light traffic volume on the main road, there was only one lane available for vehicles traveling in one direction. Sidewalks were present on both sides, adding to the pedestrian's sense of safety and ease as he made his way across.\",\n          \"The pedestrian is a male in his 20s, approximately 170 cm tall, wearing a black T-shirt and black slacks. He is standing diagonally to the right, in front of the vehicle, with his body oriented diagonally to the right as well, in the same direction as the vehicle. The pedestrian is close to the vehicle, and his line of sight is immediately above. He is unaware of the vehicle's presence. The overall environment is cloudy and dark, with a dry asphalt road surface. The road is a residential road with two-way traffic and an intersection with a signal. There are no sidewalks on both sides, and there are no roadside strips; however, there are street lights present. The pedestrian's general action is to stand still, but his abnormal action is lying stretched out. The traffic volume is light.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption_vehicle\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"The vehicle was moving at a constant speed when it suddenly came to a stop. Positioned on the right side of a pedestrian, the vehicle was in close proximity to them. The pedestrian was clearly visible within the vehicle's field of view. The vehicle's speed was at 0km/h, indicating a complete halt. In terms of the environment conditions, the pedestrian was a male in his 50s, standing at a height of 170 cm. He was wearing glasses, a black hat, a blue T-shirt, and blue slacks. The event took place in an urban area on a weekday with clear weather and bright brightness. The road surface was dry and level, consisting of asphalt. The traffic volume was light as the vehicle was traveling on a main road with a single one-way lane and sidewalks on both sides.\",\n          \"The vehicle, traveling at a speed of 20 km/h, was positioned behind and to the left of the pedestrian. The vehicle was at a close distance to the pedestrian and had a clear view of them. It started to turn right. The vehicle and the pedestrian were on a residential road, which had two-way traffic. The road was dry, level, and made of asphalt. The vehicle and pedestrian were at an intersection with a traffic signal in place. The environment conditions indicated that the pedestrian was a male in his 20s, with a height of 170 cm. He was wearing a black T-shirt and black slacks. The weather was cloudy, and it was dark outside. The road had light traffic volume and was not illuminated by street lights. There was no sidewalk and no roadside strip on either side of the road.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.2318524181131085,\n        \"min\": 19.633,\n        \"max\": 39.173,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          20.954,\n          29.418\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.0901498714766245,\n        \"min\": 19.967,\n        \"max\": 39.973,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          21.021,\n          30.412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elaboramos un conteo de palabras para determinar algunas palabras relevantes a identificar en una clasificacion."
      ],
      "metadata": {
        "id": "An8-RXR9G7Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "h_ZJgWR8jUt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TO DO: Hacer una funcion que itere para cada una de las labels y aplicar la funcion a cada columna\n",
        "#text_colums = ['caption_pedestrian', 'caption_vehicle']"
      ],
      "metadata": {
        "id": "qnn1cqxJpF42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#labels = ['prerecognition', 'recognition', 'judgement', 'action', 'avoidance']"
      ],
      "metadata": {
        "id": "M-h-RiDdpF7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_kw_action_pedestrian = df[df['label'] == 'action']['caption_pedestrian'].apply(extract_keywords).tolist()"
      ],
      "metadata": {
        "id": "SvTfh6KBpGBJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_act = {}\n",
        "\n",
        "for l in list_kw_action_pedestrian:\n",
        "  for w in l:\n",
        "    if w in dict_act:\n",
        "      dict_act[w] += 1\n",
        "    else:\n",
        "      dict_act[w] = 1"
      ],
      "metadata": {
        "id": "d1tQs9q1rIUF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_act = pd.DataFrame.from_dict(dict_act, orient='index', columns=['count'])"
      ],
      "metadata": {
        "id": "4ow4_NG1rIW8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_act = df_act.sort_values(by='count', ascending=False).reset_index().rename(columns={'index': 'word'})"
      ],
      "metadata": {
        "id": "bWiNs_nHrIZi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_act.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JFtsuQJDGVN5",
        "outputId": "35102c04-3d35-48f3-cb98-cf52b70613c8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         word  count\n",
              "0     vehicle  12684\n",
              "1        road  12171\n",
              "2  pedestrian  11491\n",
              "3       front   4745\n",
              "4     traffic   4489"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72b01f65-f275-4091-8bb8-df3267c71980\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vehicle</td>\n",
              "      <td>12684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>road</td>\n",
              "      <td>12171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pedestrian</td>\n",
              "      <td>11491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>front</td>\n",
              "      <td>4745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>traffic</td>\n",
              "      <td>4489</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72b01f65-f275-4091-8bb8-df3267c71980')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-72b01f65-f275-4091-8bb8-df3267c71980 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-72b01f65-f275-4091-8bb8-df3267c71980');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1a7b8727-9f05-4a7d-9e5f-491771e8ec33\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1a7b8727-9f05-4a7d-9e5f-491771e8ec33')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1a7b8727-9f05-4a7d-9e5f-491771e8ec33 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_act",
              "summary": "{\n  \"name\": \"df_act\",\n  \"rows\": 1924,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1924,\n        \"samples\": [\n          \"congested\",\n          \"mindset\",\n          \"steps\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 625,\n        \"min\": 1,\n        \"max\": 12684,\n        \"num_unique_values\": 293,\n        \"samples\": [\n          574,\n          33,\n          1397\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta parte consideramos palabras comunes en ingles relacionadas a la seguridad vial. Como vemos la mayoria aparecen en el las descripciones de los videos."
      ],
      "metadata": {
        "id": "HXVfcZEfGm70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_act[df_act['word'].isin(['accident', 'caution', 'collision', 'crash', 'danger', 'emergency', 'hazard', 'injury', 'prevention', 'risk', 'safety', 'warning'])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "C9kNC0dUGiZE",
        "outputId": "c43a1ac0-b2a0-4e90-f4d8-8bf79d21d589"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           word  count\n",
              "219     caution    120\n",
              "273      safety     72\n",
              "397      danger     29\n",
              "441        risk     24\n",
              "940   collision      4\n",
              "1029   accident      3\n",
              "1179     hazard      2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ef16b24c-bfa5-4ae6-a281-84f72f08a3cb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>caution</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>safety</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>danger</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>risk</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>940</th>\n",
              "      <td>collision</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029</th>\n",
              "      <td>accident</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1179</th>\n",
              "      <td>hazard</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef16b24c-bfa5-4ae6-a281-84f72f08a3cb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ef16b24c-bfa5-4ae6-a281-84f72f08a3cb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ef16b24c-bfa5-4ae6-a281-84f72f08a3cb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-51f5dc5d-78ef-4a9e-acbc-7a55dd43a5f9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-51f5dc5d-78ef-4a9e-acbc-7a55dd43a5f9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-51f5dc5d-78ef-4a9e-acbc-7a55dd43a5f9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_act[df_act['word']\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"caution\",\n          \"safety\",\n          \"accident\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 44,\n        \"min\": 2,\n        \"max\": 120,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          120,\n          72,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Modelado de los datos**"
      ],
      "metadata": {
        "id": "yllhAAPfFOMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline('zero-shot-classification', model = 'facebook/bart-large-mnli')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385,
          "referenced_widgets": [
            "72e00b22858044fab75a9c91111f1c6e",
            "303b219a5da348ab93fdbef0be6079cb",
            "69211095d6114bada832c8dfde3fd585",
            "389c1e4f78cf4f9c87fcd41d9a4aea25",
            "3284bcdc0c7f4d57b8e2df67647b3784",
            "7755ecc9b689480cb8e4e779bcb7432b",
            "e5bc537ea5554ee3936d1731abb03dfb",
            "74e8a4c2cc794023a1e7f77db4492834",
            "cdd322d9a8d2409592f64b40eda42576",
            "1df438ae6e7a41b693555a92e5fe0a20",
            "2f387c85c1614a1487fe546f8cd2c926",
            "151e5c45353543abb01953a0e4446544",
            "09f204564e7c4209af77908a2bfdccef",
            "7af8dcd3fa404082a67587c3fd659956",
            "d535fcca763d498297fcee1386bf6a67",
            "b60f8ece4a4d4aaba1b5ddb38746cd9d",
            "2a3723fce3c04394a41cac438ef6917e",
            "bbece02895b746e987166e96bf65b64c",
            "f932a6b860b345a18bfb84107a0ca172",
            "f986ba487f6b4aa3824b4a423f306d50",
            "0ffda4f46b30439692ed74cb6e049588",
            "9066121fee874fb890f3bfc4a3badb24",
            "5cdea4f9c01042a3a76cb838bd951ed4",
            "10ee94b4f1d44d518353a7bdc3702c5c",
            "392210b46c544433ad0a283d36dbe5f3",
            "1cb332f82d944c4fb2ab531803d97bfd",
            "0ca78f80cd0d4e1ab09013add09469d5",
            "7aafdd411e4c495a93043d651334861e",
            "7016869df29d40338ac629bfbfe39e6a",
            "3bc8073ff6c6482fb4fe01f7d6a9205c",
            "c11cfed723a64f1e9885e3992e7e2727",
            "f7a762bbf5e546e2bc21a78bd4fa5c39",
            "147cbf3f7efa4d9d8786d2a6c06dd14a",
            "b91224d75e6841718066ba9d4936dae9",
            "b4e6b40f643848a5a9778c38855b16cd",
            "825f5204d2674bd68a3c8c3d20e23dda",
            "ae2bc7c7c0844b0e91cbb236198cb020",
            "a512b5d3dc4d4e5897725443cf7a3a13",
            "993ee3485be8427fa5be70029a4c2b6e",
            "953a00d37d1a4e42943d6d7dc1ab0e31",
            "814136ee3c6847768e89906d9b9daaf6",
            "d5a3c77847b448ecad38bb01c5b75d3c",
            "33f59d2f04e34fe8ace9a231a9aa5eb1",
            "90e63832924049de8784bead117d73e4",
            "5bcf5fc10aa0466887d58d62308493a9",
            "d1356f9c3ea24f51b122072cc5c18323",
            "d1a89270dceb4739a11ad5dbd375f33a",
            "6d18a1a309a84ebb8b340eeb3d97d5de",
            "cadb03b57071415792de9ed72030821d",
            "2def0a3eee704d91859307317f7297bb",
            "9a1aa800c79748929421a73cb2caae96",
            "29b85452178c4a00a3d715f8401905a3",
            "02f86163aef64d67a121b9c932193fc7",
            "a2efe9b3ae6b4ef2b4715a0df8c1e727",
            "4bd51cb48e3a41d2a3e8b41aaba83538",
            "74c37769ad1142ff838f70bae34ffe58",
            "38db0dc3b729423bbd78b67272297cc8",
            "6e01959cce0a4b40870af8587f243d83",
            "5a86880e1bfb4b6dbd3d992c10fcc264",
            "5f3076c54a9f4be5ad6fb78f6a30e497",
            "f36abb325dd344e08ef0ee2b27f66c3b",
            "b9b995e4f27140f2b814222151d1abf6",
            "6d8d4dc7b944470eb2b29ae1570052d3",
            "cb4e02c71d7b4c4dbf2ab04d29261f22",
            "d99e7db7f87b4d65bbbf4890072cd6ba",
            "7e5d1d9a0ccd4ea09f0823862dfa7471"
          ]
        },
        "id": "CchWwWsJjldj",
        "outputId": "17266b78-fa19-4635-91f6-8ddc31e29147"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72e00b22858044fab75a9c91111f1c6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "151e5c45353543abb01953a0e4446544"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cdea4f9c01042a3a76cb838bd951ed4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b91224d75e6841718066ba9d4936dae9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5bcf5fc10aa0466887d58d62308493a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74c37769ad1142ff838f70bae34ffe58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consideramos las palabras identificadas en la seccion anterior para clasificar los textos."
      ],
      "metadata": {
        "id": "HjbLP1SgHTTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_labels = ['accident', 'caution', 'collision', 'crash', 'danger', 'emergency', 'hazard', 'injury', 'prevention', 'risk', 'safety', 'warning']"
      ],
      "metadata": {
        "id": "5HYPqFv1pYsb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Muestra"
      ],
      "metadata": {
        "id": "bnhxRlanHk3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Condeideramos solo los textos correspondientes al label 'action'\n",
        "df_sample = df[df['label'] == 'action'].sample(frac=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "rJOMFSJwpe63"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample['class'] = df_sample['caption_pedestrian'].apply(classify_text)"
      ],
      "metadata": {
        "id": "4Ey3JcD-pyz_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tiempo de ejecucion promedio por texto: 4.6 segundos con un GPU A100"
      ],
      "metadata": {
        "id": "h_1jlFocIacN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Resultados**"
      ],
      "metadata": {
        "id": "cH5mSzpuHw5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample.groupby('class').count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KD8ZtW_5BWN_",
        "outputId": "64e8b56b-a61c-4e14-eec3-32aded20fc84"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id  video_name  fps  label  caption_pedestrian  caption_vehicle  \\\n",
              "class                                                                       \n",
              "caution   49          49   49     49                  49               49   \n",
              "hazard     1           1    1      1                   1                1   \n",
              "risk     266         266  266    266                 266              266   \n",
              "safety    24          24   24     24                  24               24   \n",
              "\n",
              "         start_time  end_time  \n",
              "class                          \n",
              "caution          49        49  \n",
              "hazard            1         1  \n",
              "risk            266       266  \n",
              "safety           24        24  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc466566-d580-4edc-b37a-89f7750a0935\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>video_name</th>\n",
              "      <th>fps</th>\n",
              "      <th>label</th>\n",
              "      <th>caption_pedestrian</th>\n",
              "      <th>caption_vehicle</th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>caution</th>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hazard</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>risk</th>\n",
              "      <td>266</td>\n",
              "      <td>266</td>\n",
              "      <td>266</td>\n",
              "      <td>266</td>\n",
              "      <td>266</td>\n",
              "      <td>266</td>\n",
              "      <td>266</td>\n",
              "      <td>266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>safety</th>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc466566-d580-4edc-b37a-89f7750a0935')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fc466566-d580-4edc-b37a-89f7750a0935 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fc466566-d580-4edc-b37a-89f7750a0935');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-97428e8b-ef7a-4ff1-b49f-4a3790ea2fd7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-97428e8b-ef7a-4ff1-b49f-4a3790ea2fd7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-97428e8b-ef7a-4ff1-b49f-4a3790ea2fd7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_sample\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"hazard\",\n          \"safety\",\n          \"caution\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 122,\n        \"min\": 1,\n        \"max\": 266,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          24,\n          49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video_name\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 122,\n        \"min\": 1,\n        \"max\": 266,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          24,\n          49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 122,\n        \"min\": 1,\n        \"max\": 266,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          24,\n          49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 122,\n        \"min\": 1,\n        \"max\": 266,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          24,\n          49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption_pedestrian\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 122,\n        \"min\": 1,\n        \"max\": 266,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          24,\n          49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption_vehicle\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 122,\n        \"min\": 1,\n        \"max\": 266,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          24,\n          49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 122,\n        \"min\": 1,\n        \"max\": 266,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          24,\n          49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 122,\n        \"min\": 1,\n        \"max\": 266,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          24,\n          49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "afGhE1HDIRR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}